# 향후 연구 제안: 외부 LLM을 활용한 보상 재설계 (Reward Shaping)

기존 연구가 규칙 기반의 보상 시스템을 통해 모델의 '정답률'을 높이는 데 집중했다면, 다음 단계로는 **'풀이 과정의 품질'** 자체를 평가하여 모델의 추론 능력을 질적으로 향상시키는 방안을 고안해보았습니다.

이를 위해 외부의 고성능 LLM(예: Gemini)을 '지능적인 평가자(Intelligent Judge)'로 사용하는 **보상 재설계(Reward Shaping)** 방법론의 코드가 구현되었습니다. (`저도 이 연구를 진행하려다 On Premise 모델로 시도 해보는 걸로 돌려서 코드가 최적화 되어있진 않습니다... 필요할 것 같은 함수들 이것저것 짜놨는데 불필요한 부분들이 있으면 제거하시고 사용하시길 바랍니다. `)

**단, 실제 실험은 아직 수행되지 않았으므로, 이 연구를 하시게 된다면 API 발생 비용을 고려해서 하이퍼파라미터와 API 호출과 같은 부분을 잘 최적화하고 스텝 수 10~20회 정도 먼저 해보시고 로깅을 확인하신 후에 본 학습을 하시길 권장드립니다.**

## 1. 제안 배경: 보상 재설계(Reward Shaping)란?

보상 재설계는 최종 결과(성공/실패)에 대해서만 보상을 주는 대신, 목표에 도달하는 **과정 중에 추가적인 보상 신호**를 주어 에이전트의 학습을 더 효율적으로 안내하는 기법입니다. 즉, 정답 일치 여부와 같은 최종 결과뿐만 아니라, 풀이 과정의 논리성에 대해서도 추가적인 보상을 부여하는 것을 의미합니다.

-   **희소한 보상(Sparse Reward) 문제**: 현재의 규칙 기반 보상 시스템은 최종 답이 맞았을 때만 높은 점수를 부여합니다. 모델이 수많은 단어를 생성하고 복잡한 논리 전개를 펼친 끝에 단 하나의 보상 신호를 받게 되는데, 이는 모델이 "무엇을 잘했고 무엇을 못했는지" 구체적으로 알기 어려운 **'희소한 보상'** 문제입니다.
-   **조밀한 보상(Dense Reward) 제공**: 보상 재설계는 이 문제를 해결하기 위해 **"풀이 과정은 논리적이었는가?", "접근 방식이 효율적이었는가?"** 와 같은 중간 과정의 품질을 평가하여 더 **'조밀한 보상'**을 제공합니다. 이는 모델이 올바른 방향으로 나아가도록 세밀하게 안내하는 역할을 합니다.

## 2. 구현된 워크플로우: Gemini를 평가자로 활용하기

이 방법론을 위해, 외부 LLM인 `Gemini`를 지능적인 평가자로 사용하여 풀이 과정의 품질을 평가하고, 이를 보상 신호로 변환하는 코드(`RL_GRPO_reward shaping.ipynb`)가 구현되었습니다. 전체적인 흐름은 다음과 같습니다.

1.  🤖 **풀이 생성 (Action)**
    * 훈련 대상인 **`AtomicGPT`(Student)**가 주어진 수학 문제에 대해 `reasoning(추론 과정)`과 `answer(정답)`를 포함한 답변을 생성합니다.

2.  🧐 **풀이 과정 추출**
    * `reasoning_quality_reward_func` 함수는 생성된 답변에서 `<reasoning>` 태그 안의 텍스트, 즉 모델의 문제 해결 논리를 추출합니다.

3.  📝 **평가 프롬프트 구성**
    * 추출된 풀이 과정, 원본 문제, 그리고 실제 정답을 조합하여 **`Gemini`(Judge)**에게 보낼 상세한 평가 프롬프트를 동적으로 생성합니다.
    * 이 프롬프트는 Gemini에게 '수학 교육 전문가'의 역할을 부여하고, 명확한 채점 기준(논리성, 효율성 등)에 따라 0~5점 척도로 점수를 매기도록 지시합니다.

4.  ⚖️ **외부 평가 수행**
    * 구성된 프롬프트를 `Gemini` API로 전송하여 평가를 요청합니다. `Gemini`는 주어진 기준에 따라 학생의 풀이 과정을 분석하고, 평가 결과를 JSON 형식으로 반환합니다.
    * (참고: 이 방식은 기존 연구에서 `Math-500` 데이터셋의 복잡한 답변을 평가하기 위해 `Gemini`를 활용했던 것과 유사한 접근법입니다.)

5.  📊 **보상 신호 변환**
    * API 응답(JSON)에서 `reasoning_score`를 파싱합니다. 이 점수는 이제 "풀이 과정의 품질"을 나타내는 정량적 지표가 됩니다.
    * 이 점수를 0~1 사이로 정규화하고, `correctness_reward_func` (정답 여부) 및 `format_reward_func` (형식 준수) 등 다른 보상들과 가중합(`combined_reward_func`)하여 최종 "재설계된 보상(Shaped Reward)"을 계산합니다.

6.  💡 **정책 업데이트 (Learning)**
    * 최종적으로 계산된 이 **재설계된 보상**을 사용하여 GRPO 알고리즘을 통해 `AtomicGPT`의 정책(모델의 LoRA 가중치)을 업데이트합니다.

## 3. 장점과 한계

이 접근 방식은 최고의 성능을 목표로 할 때 매우 합리적인 방법론이지만, 비용과 속도라는 현실적인 트레이드오프를 고려해야 합니다.

| 장점 👍                                                                                                     | 한계 👎                                                                                                   |
| :---------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------- |
| **고품질 피드백**: "논리성"이나 "창의성"처럼 규칙으로 정의하기 어려운 추상적인 품질을 평가할 수 있습니다.     | **API 비용**: 외부 API를 호출할 때마다 비용이 발생하여 전체 훈련 비용이 증가합니다.                        |
| **성능 향상 잠재력**: 인간 전문가의 평가와 유사한 수준의 피드백은 모델 성능을 획기적으로 향상시킬 수 있습니다. | **훈련 속도 저하**: API 호출에 수반되는 네트워크 지연(latency)으로 인해 훈련 파이프라인의 속도가 느려집니다. |
| **학습 효율성**: 조밀한 보상 신호는 모델이 더 빠르고 안정적으로 올바른 방향을 학습하도록 돕습니다.             | **외부 서비스 의존성**: Gemini 서비스의 상태나 정책 변경에 따라 훈련이 영향을 받을 수 있습니다.              |

## 4. 향후 연구를 위한 주요 고려사항 및 제언

본 연구를 본격적으로 진행하기 전, 아래의 현실적인 문제들을 반드시 검토하고 진행 여부를 결정해야 합니다.

-   **[비용] API 호출 비용**: 훈련의 모든 스텝마다 외부 API를 호출하는 것은 상당한 금전적 비용을 발생시킬 수 있습니다. 전체 훈련에 필요한 예상 비용을 사전에 산출하고, 그 비용에 대해 실장님께 먼저 말씀 드리시길 권장드립니다.
-   **[속도] 훈련 시간**: API 호출 시 발생하는 네트워크 지연(latency)으로 인해 전체 훈련 시간이 수 배에서 수십 배까지 길어질 수 있습니다. 이는 연구 개발의 속도를 크게 저하시키는 요인이 될 수 있습니다.
-   **[편향] 평가자 모델의 의존성**: 훈련 결과가 전적으로 '평가자'로 사용되는 외부 LLM의 성능과 편향에 좌우됩니다. 만약 평가자 모델이 특정 방식의 풀이를 선호하거나 실수를 한다면, 학생 모델 역시 그 편향이나 실수를 그대로 학습하게 될 위험이 있습니다.
-   **[재현성] 실험의 일관성**: 외부 API는 언제든지 업데이트될 수 있습니다. 만약 평가자 모델이 업데이트되면, 이전과 동일한 조건에서 실험을 재현하는 것이 불가능해질 수 있습니다.

### 최종 제언: 연구 진행 여부 결정을 위하여

위험 요소를 고려했을 때, 전면적인 학습을 시작하기 전에 다음과 같은 **사전 타당성 검토**를 수행할 것을 권장합니다.

1.  **소규모 테스트 실행**: 전체 데이터가 아닌, 아주 적은 수의 샘플(예:4개)과 짧은 스텝(예: 10 steps)으로 훈련을 실행하여 **단계별 소요 시간과 예상 비용**을 측정합니다.
2.  **평가 품질 검증**: 소규모 테스트에서 외부 LLM(Judge)이 반환한 평가 점수와 피드백 내용을 수동으로 검토합니다. 과연 평가가 일관성이 있고, 합리적인 기준에 따라 이루어지는지 확인해야 합니다.
3.  **손익 분석**: 사전 테스트 결과를 바탕으로, 예상되는 총비용, 총 소요 시간, 그리고 연구를 통해 얻을 수 있는 성능 향상의 가치를 종합적으로 비교하여 최종적으로 **이 연구를 계속 진행할지** 결정해야 합니다.

### 결론 

현재 이 '보상 재설계' 접근 방식은 아이디어 구현 및 코드 작성 단계에 있으며, **실제 실험을 통해 그 효과가 검증되지는 않았습니다.** 하지만 향후 연구를 원활하게 진행할 수 있도록 `RL_GRPO_reward shaping.ipynb`에 로직이 구현되어 있습니다.

모델은 API 호출이 되는 모델을 사용하시면 되지만, 제미나이 모델이 가장 저렴할 것입니다. 다른 모델을 사용하고 싶으시다면 `API Pricing`을 잘 확인하시고 라이브러리 설치후에 엔드포인트를 잘 지정해서 사용하시면 될 것입니다. (현재로서는 "Gemini 2.5 Flash"가 가장 좋아보입니다.)

이 주제로 후속 연구를 진행하고자 하신다면 실장님과 협의해 대략적인 API 비용을 확인하시고 허가해주신다면 본 문서에서 제안된 **사전 타당성 검토**를 먼저 수행하여 비용, 시간, 평가 품질의 실효성을 판단한 후, **준비된 코드를 기반으로 연구를 확장해 나가는 것**을 권장합니다.

---
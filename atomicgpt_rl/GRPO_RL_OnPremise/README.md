## 🔬 접근 방식 3: On-Premise Judge 모델 (Self-Contained RL)

이 접근 방식은 **외부 API의 비용 및 속도 문제**와 **규칙 기반 보상의 낮은 품질**이라는 두 가지 한계를 동시에 해결하기 위한 대안입니다. 실제 실험을 통해 얻은 교훈과 한계점을 바탕으로, 후속 연구를 위한 구체적인 방향을 제시합니다. **전체적인 워크플로우는 basic한 GRPO RL과 유사하므로 참고하시길 바랍니다.** 

또한 외부망 서버(23번, 26번, 29번)에서 작업하신다면 보안 문제로 인해 Hugging Face에서 모델을 불러올 수 없습니다. 서버에 있는 모델이 아니면 사용할 수 없으므로 클라우드 서비스에서 연구하셔야 합니다. (보안 관련된 데이터 셋 유의)

### 개요

-   **목표**: 외부 API 의존성을 제거하고, 내부 서버(On-Premise)에서 실행되는 Judge 모델을 활용하여 **데이터 보안, 비용 효율성, 완전한 통제권**을 확보합니다.
-   **핵심 아키텍처**:
    -   **Student 모델**: 훈련 대상 모델 (`KAERI-MLP/gemma2-Korean-AtomicGPT-9B`). 4-bit 양자화와 LoRA를 적용하여 효율적으로 학습합니다.
    -   **Judge 모델**: 평가자 모델 (`meta-llama/Llama-3.1-8B-Instruct`). Student 모델과 동일한 GPU 환경에서 로드되어, 생성된 답변을 실시간으로 평가하고 보상을 생성합니다. (평가자 모델은 다른 모델로 바꾸셔도 됩니다. `Qwen/Qwen3-8B`도 추천드립니다.)

### 접근 방식 비교 분석

| 특징 | 규칙 기반 RL (접근 방식 1) | 외부 LLM 활용 (접근 방식 2) | On-Premise LLM 활용 (접근 방식 3) |
| :--- | :--- | :--- | :--- |
| **평가 주체** | 직접 코딩한 로직 (Python) | 외부 상용 API (Gemini 등) | 내부에서 호스팅하는 모델 (Llama 등) |
| **평가 품질** | **낮음** (논리적 흐름 평가 불가) | **높음** (미묘한 뉘앙스, 논리 평가 가능) | **중간** (Judge 모델 성능에 전적으로 의존) |
| **비용** | **없음** | **높음** (API 호출당 비용 발생) | **낮음** (초기 하드웨어 구축 비용 외 없음) |
| **속도** | **빠름** | **느림** (네트워크 지연 발생) | **중간** (내부 추론 속도에 따라 결정) |
| **데이터 보안** | **높음** (완전한 내부망) | **낮음** (데이터 외부 전송) | **높음** (완전한 내부망) |
| **VRAM 요구량** | **낮음** (Student 모델만 필요) | **낮음** (Student 모델만 필요) | **높음** (Student + Judge 모델 동시 로드) |

### 워크플로우 및 현실적 타협점

1.  🤖 **응답 생성**: **Student 모델**이 문제에 대한 풀이 과정을 생성합니다. 
2.  ⚖️ **내부 평가 수행**: `agent_reward_func` 함수가 Student의 답변, 원본 문제, 정답을 조합하여 **내부 Judge 모델**에 전달하고, 평가 점수를 받습니다.
    -   **초기 구상**: 처음에는 '정답 정확도'와 '풀이 과정의 논리성'을 별도로 평가하여 보상을 차등 지급하고자 했습니다.
    -   **현실적 타협**: 하지만 한정된 GPU 자원에서는 고성능의 큰 Judge 모델을 사용하기 어렵습니다. 소형 Judge 모델은 복잡한 다중 평가 요구를 잘 따르지 못하는 경향이 있어, 불가피하게 **"정답(50%)과 풀이(50%)를 종합적으로 고려하여 단일 점수를 달라"**는 단순한 프롬프트로 타협했습니다. 
3.  📊 **보상 변환 및 정책 업데이트**: Judge가 반환한 점수를 0~1로 정규화하여 Student 모델의 정책을 업데이트합니다.

### 주요 도전 과제 및 현 연구의 한계점

-   **[하드웨어 제약과 Judge 모델의 한계]**:
    -   **VRAM 문제**: Student와 Judge, 두 개의 LLM을 동시에 로드하는 것은 매우 높은 VRAM을 요구합니다. 이는 이 접근법의 가장 큰 진입 장벽입니다.
    -   **소형 Judge 모델의 신뢰성 문제**: 현재 사용된 소형 Judge 모델은 프롬프트 지시를 완벽하게 따르지 않는 문제가 빈번했습니다. 평가 점수만 반환하도록 요청했음에도, ` ```python `과 같은 불필요한 텍스트나 코드 블록을 출력하여 보상 파싱 오류를 일으키고 학습을 방해하는 경우가 많았습니다.
    -   **미완의 최적화**: 이 문제를 해결하기 위해 Judge 모델의 최대 생성 토큰을 제한하고(예 : new_max_token = 5) 프롬프트를 수정하는 방법을 시도했지만, 근본적인 해결책이 되지는 못했습니다. 이는 이 연구 방식이 가진 **해결문제**입니다.

-   **[보상 해킹(Reward Hacking)의 위험]**:
    -   강화학습의 고질적인 문제로, Student 모델이 실제 추론 능력을 향상시키는 대신 **Judge 모델로부터 높은 점수를 받는 요령(꼼수)을 학습**할 수 있습니다. 예를 들어, 특정 키워드나 문장 구조를 사용하면 Judge 모델이 점수를 후하게 준다는 사실을 발견하고 그 패턴만 반복하는 것입니다.
    -   이는 겉보기에는 보상 점수가 오르는 것처럼 보이지만, 실제 모델의 성능은 정체되거나 오히려 저하되는 '보상 해킹' 현상으로 이어질 수 있으며, 이를 해결하는 것이 중요 과제입니다.
    -   **얼리 스탑(Early Stopping)** 이 방법이 될 수 있습니다. 평균누적보상에 대한 값은 지속적으로 상승하지만, `agent_reward_func` 함수에 대한 누적보상은 상승하지 않는다면 보상 해킹을 의심하고 `Early Stopping`을 해보시는 것도 방법이 될 수 있습니다.

### 향후 개선 방향

이러한 한계점을 극복하고 연구를 발전시키기 위해 다음 두 가지 방향을 제안합니다.

1.  **더 큰 Judge 모델 사용 (GPU 자원 확보 시)**: 충분한 VRAM이 확보된다면, 더 크고 성능 좋은 Judge 모델(예: 30B 이상)을 사용하는 것이 근본적인 해결책이 될 수 있습니다. 큰 모델은 복잡한 지시를 더 잘 따르므로, '정답'과 '풀이'를 분리하여 평가하는 초기 구상을 실현할 수 있습니다.

2.  **평가 역할 분리 (하이브리드 접근법)**:
    -   **정답 평가**: basic GRPO RL에 있는 제가 만든, 정답추출 알고리즘을 사용하시는 것도 대안이 될 수 있습니다.(제 체감 상으로는 이게 더 정답에 대한 보상을 합리적으로 주는 것 같습니다..)
    -   **풀이 과정 평가**: **On-Premise Judge 모델은 '풀이 과정의 논리성'이라는 주관적인 부분의 평가에만 집중**하도록 역할을 분담합니다.
    -   이 방식은 Judge 모델의 부담을 줄여 신뢰도를 높이고, 보상 해킹의 위험을 감소시킬 수 있는 효과적인 대안이 될 수 있습니다.
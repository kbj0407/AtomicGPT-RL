# 🚀 RL 기반 AtomicGPT 추론능력 향상

## 🧠 강화학습(Reinforcement Learning)이란?

강화학습은 **에이전트(Agent)**가 주어진 **환경(Environment)** 내에서 특정 **행동(Action)**을 취하고, 그 결과로 **보상(Reward)**을 받으며 최적의 **정책(Policy)**을 스스로 학습하는 방법입니다.

이해를 돕기 위해 축구 선수 예시를 통해 주요 개념을 살펴보겠습니다.

### 주요 개념

| 개념 | 설명 | 축구 선수 예시 ⚽️ |
| :--- | :--- | :--- |
| **에이전트 (Agent)** | 학습의 주체 (AI) | 축구 선수 |
| **환경 (Environment)** | 에이전트가 처한 상황 | 축구공을 가지고 있는 상황, 경기장 |
| **행동 (Action)** | 환경 내에서 취할 수 있는 행동 | 슛, 패스, 드리블 |
| **정책 (Policy)** | 특정 환경에서 어떤 행동을 할지 결정하는 전략 | 골대 근처에서는 `슛`, 앞에 수비수가 있으면 `패스` |
| **보상 (Reward)** | 행동에 대한 긍정적/부정적 피드백 | **득점**: `+1`, **드리블 성공**: `+0.5`, **패스 실패**: `-0.5` |
| **어드밴티지 (Advantage)** | 특정 행동이 그 상황의 '평균적인 기대치'보다 얼마나 더 나은 선택인지를 나타내는 상대적 값 | 골대 앞에서 무리하게 슛하면 득점 확률이 낮아 **어드밴티지는 음수(-)**. 반면, 더 좋은 위치의 동료에게 패스하는 것이 득점 확률을 높인다면 **어드밴티지는 양수(+)** 가 된다. |

> **핵심 목표**: 에이전트는 수많은 행동과 그에 따른 보상을 경험하며, 누적 보상을 최대로 받을 수 있는 **최적의 정책(Optimal Policy)**을 스스로 찾아나가는 것을 목표로 합니다.

### 🎯 연구의 핵심 과제: 정교한 Policy 설계의 어려움

인간에게는 "좋은 글"이나 "논리적인 풀이"를 판단하는 것이 직관적이지만, AI는 이를 수치적으로 인지하기 어렵습니다. 따라서 강화학습의 성능은 **어떻게 보상을 설계하느냐** 에 따라 크게 좌우됩니다.

-   **정책의 중요성**: 결국 보상 시스템이 정책을 결정하므로, 최종 목표는 **정교한 보상 시스템을 통해 인간의 의도에 부합하는 최적의 정책을 학습**시키는 것입니다.
-   **설계의 어려움**: 인간과 달리 AI는 직관적인 판단 능력이 없기 때문에, 복잡하고 미묘한 품질을 평가할 수 있는 보상 시스템을 설계하는 것이 이 연구의 핵심 도전 과제입니다.

---

## 📝 프로젝트 개요

이 프로젝트는 **GRPO(Group Relative Policy Optimization)**라는 강화학습 알고리즘을 사용하여 `AtomicGPT-gemma2-9B` 모델의 수학적 추론 능력(GSM8K 데이터셋 기준)을 향상시키는 것을 목표로 합니다.

Parameter-Efficient Fine-Tuning(PEFT) 기법인 **LoRA(Low-Rank Adaptation)** 와 **4-bit 양자화(Quantization)** 를 적용하여 제한된 VRAM 환경에서도 효율적으로 모델을 훈련할 수 있도록 설계되었습니다.

---
## 🤖 알고리즘 심층 분석: GRPO (Group Relative Policy Optimization)

이 프로젝트는 전통적인 강화학습 알고리즘인 PPO(Proximal Policy Optimization) 대신, 더 효율적인 **GRPO**를 핵심 알고리즘으로 채택합니다. GRPO는 학습 비용, 특히 VRAM 사용량을 크게 절감하면서도 안정적인 정책 학습을 가능하게 합니다.

### PPO의 작동 방식과 한계

PPO는 강화학습의 표준으로 널리 사용되지만, 두 개의 주요 모델을 필요로 합니다.

1.  **정책 모델 (Policy Model / Actor)**: 어떤 행동(단어 생성)을 할지 결정합니다.
2.  **가치 모델 (Value Model / Critic)**: 현재 상태가 얼마나 좋은지, 즉 미래에 받을 보상의 기댓값을 예측합니다.

PPO는 **가치 모델(Critic)**이 예측한 값을 기준으로 **어드밴티지(Advantage)**, 즉 특정 행동이 평균보다 얼마나 더 좋은지를 계산합니다.

-   **한계**: 가치 모델은 보통 정책 모델과 거의 동일한 크기를 가집니다. 이는 **두 개의 대규모 언어 모델을 동시에 VRAM에 올려야 함**을 의미하며, 이는 엄청난 메모리 요구량과 훈련 비용 증가로 이어집니다.

### GRPO: Critic 없이 똑똑하게 학습하기

GRPO는 이러한 PPO의 한계를 **가치 모델(Critic)을 제거**하는 혁신적인 방식으로 해결합니다.

#### 핵심 아이디어: 그룹 내 상대평가

가치 모델이 하던 '절대적인 좋음'을 예측하는 대신, GRPO는 **'상대적인 좋음'**을 평가합니다.

1.  **그룹 샘플링 (Group Sampling)**: 하나의 프롬프트(`q`)에 대해, 모델은 **여러 개의 다른 결과(`o1, o2, ..., oG`)**를 생성합니다.
2.  **그룹 내 보상 계산**: 생성된 각 결과에 대해 보상(`r1, r2, ..., rG`)을 계산합니다.
3.  **상대적 어드밴티지(Relative Advantage) 계산**: 어드밴티지 `A_i`는 그룹 전체의 평균 및 표준편차와 비교하여 계산됩니다.

    $$
    A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \dots, r_G\})}{\text{std}(\{r_1, r_2, \dots, r_G\})}
    $$

    -   **직관적 의미**: 특정 결과(`o_i`)가 좋은 행동이었는지는 절대적인 점수가 아니라, **"같은 상황에서 시도한 다른 결과들보다 평균적으로 얼마나 더 나았는가?"**로 판단합니다. 이는 매우 효율적이고 안정적인 평가 방식입니다.

### PPO vs. GRPO 비교

| 특징 | PPO (Proximal Policy Optimization) | GRPO (Group Relative Policy Optimization) |
| :--- | :--- | :--- |
| **가치 모델 (Critic)** | **필수** (VRAM 사용량 큼) | **불필요** (VRAM 사용량 적음) |
| **어드밴티지 계산** | 가치 모델의 예측값에 기반한 절대적 평가 | 그룹 내 샘플들의 평균/표준편차에 기반한 **상대적 평가** |
| **샘플링 전략** | 프롬프트 당 하나의 결과 샘플링 | 프롬프트 당 **여러 개의 결과(그룹)** 샘플링 |
| **핵심 장점** | 안정적이고 검증된 성능 | **훈련 비용 및 메모리 효율성 극대화** |
| **주요 트레이드오프** | 높은 컴퓨팅 자원 요구 | 그룹 샘플링으로 인한 약간의 추론 오버헤드 |

### GRPO의 목적 함수

GRPO의 목적 함수는 PPO와 구조적으로 매우 유사하지만, 어드밴티지 `A_i`가 그룹 상대평가 방식으로 계산된다는 점이 결정적인 차이입니다.

$$
J_{\text{GRPO}}(\theta) = \mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(O|q)} \left[ \frac{1}{G} \sum_{i=1}^{G} \min \left( \frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{old}}(o_i|q)} A_i, \text{clip}\left( \frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{old}}(o_i|q)}, 1-\epsilon, 1+\epsilon \right) A_i \right) - \beta D_{KL}(\pi_{\theta} || \pi_{ref}) \right]
$$

-   `clip(...)` 함수는 PPO와 마찬가지로 정책이 너무 급격하게 변하는 것을 막아 학습 안정성을 높입니다.
-   `D_KL(...)` 항은 현재 정책이 초기 참조 모델(SFT 모델)에서 너무 멀어지지 않도록 규제하는 역할을 합니다.

**결론적으로, 이 프로젝트에서 GRPO를 사용하는 이유는 명확합니다.** 가치 모델을 제거함으로써 제한된 GPU 자원 내에서도 대규모 언어 모델의 강화학습을 효율적으로 수행할 수 있기 때문입니다. 

[**GRPO에 대한 자세한 설명은 Deepseek-R1 논문을 참고해서 보시면 좋을 것 같습니다.**](./paper_and_poster/DeepSeek_R1.pdf)

## ⚙️ 핵심 기술 스택

-   **Model**: `AtomicGPT-gemma2-9B`
-   **Dataset**: `openai/gsm8k` (학습시키고 싶은 데이터셋을 선별해서 수정하시면 됩니다.)
-   **RL Algorithm**: `GRPO (Group Relative Policy Optimization)`
-   **Frameworks**: `PyTorch`, `Hugging Face Transformers`, `TRL`, `PEFT` 등
-   **Efficiency Tech**: 4-bit Quantization, LoRA


**연구하시는 서버(연구원 내 서버, 클라우드 컴퓨팅)에 따라서 모델 불러오는 방법과 라이브러리 버전과 같은 것들을 세팅하고 진행하시면 됩니다.**

**연구원 내 서버에서 연구하신다면, 외부망 서버에 있는 모델의 경로로 지정하시면 되고 클라우드 컴퓨팅 서비스에서 작업하신다면 허깅페이스를 통해 불러와야 합니다. `KAERI-MLP/AtomicGPT-gemma2-9B`를 입력해 모델을 로드하시면 됩니다.**

---

## 🛠️ 코드 실행 파이프라인

현재의 코드는 다음과 같은 순서로 실행되며 .

1.  **환경 설정 및 모델 로딩**:
    -   필요한 라이브러리를 설치합니다.
    -   `BitsAndBytesConfig`를 사용하여 모델을 **4-bit로 양자화**하여 로드합니다. 이는 모델이 차지하는 VRAM을 크게 줄여줍니다.
    -   `LoraConfig`를 통해 LoRA 설정을 정의하고, `get_peft_model` 함수로 모델에 적용합니다. 이를 통해 전체 모델 파라미터를 동결하고, 소수의 LoRA 파라미터만 학습하여 메모리 효율성을 극대화합니다.

2.  **데이터 준비**:
    -   `gsm8k` 데이터셋을 로드합니다. 이 데이터셋은 다양한 수학 응용 문제로 구성되어 있습니다.
    -   각 문제를 `<answer>...</answer>` 형식으로 응답하도록 유도하는 **시스템 프롬프트(System Prompt)**와 함께 구성합니다. 이는 CoT(Chain of Thought)를 유도하기 위함입니다.

3.  **보상 함수(Reward Functions) 설계**:
    -   모델이 생성한 답변을 평가하고 보상을 부여하는 여러 함수를 정의합니다. 이는 강화학습의 핵심으로, 모델의 행동을 원하는 방향으로 유도합니다. 이 프로젝트에서는 다층적 보상 시스템을 사용하여 모델을 단계적으로 안내합니다. 보상 배점은 코드에 있는 배점을 조정하셔서 사용하시면 됩니다.
    -   `correctness_reward_func`: **(핵심 보상)** 생성된 답변에서 최종 숫자를 추출하여 실제 정답과 비교합니다. 정답일 경우 가장 높은 보상을 부여하여, 모델이 '정확한 답'을 찾도록 강력하게 유도합니다. 정답을 잘 추출하는 알고리즘을 짜는 것이 핵심입니다.
    -   `strict_format_reward_func`: **(구조적 보상)** 답변이 `<answer>...</answer>`의 엄격한 XML 형식을 완벽하게 따르는지 확인합니다. 형식을 맞추면 보상을 주어, 모델이 정해진 출력 구조를 학습하게 합니다.
    -   `soft_format_reward_func`: **(유연한 구조 보상)** `Reasoning:`이나 `Answer:` 같은 키워드만 포함해도 보상을 주어, 처음에는 느슨한 형태로라도 구조를 잡도록 격려합니다.
    -   `int_reward_func`: **(기초 보상)** 답변에 숫자가 포함되기만 해도 보상을 줍니다. 이는 모델이 최소한 숫자 값을 생성하도록 유도하는 초기 단계의 힌트입니다.
    -   `xmlcount_reward_func`: **(부분적 보상)** 각 XML 태그가 존재할 때마다 작은 보상을 부여하여, 모델이 완전한 XML 구조를 갖추도록 세밀하게 유도합니다.

4.  **GRPO 트레이너 설정 및 훈련**:
    -   `GRPOConfig`를 통해 학습률, 배치 사이즈, 옵티마이저 등 훈련에 필요한 모든 하이퍼파라미터를 설정합니다.
    -   `GRPOTrainer`에 모델, 데이터셋, 보상 함수, 설정을 전달하여 훈련을 시작합니다.

---

## 📊 주요 하이퍼파라미터 설명 (`GRPOConfig`)

훈련 성능과 안정성에 큰 영향을 미치는 주요 변수들은 다음과 같습니다.

| 파라미터 | 설명 | OOM 관련성 |
| :--- | :--- | :--- |
| `per_device_train_batch_size` | 한 번의 학습 스텝에서 각 GPU가 처리하는 데이터 샘플의 수입니다. | **(높음)** 이 값을 줄이는 것이 OOM 해결의 가장 직접적인 방법입니다. |
| `gradient_accumulation_steps` | 이 값을 `N`으로 설정하면, `N`번의 스텝 동안 계산된 그래디언트를 누적한 후 한 번에 모델 파라미터를 업데이트합니다. `batch_size`를 늘리는 것과 유사한 효과를 내면서 메모리 사용량은 줄일 수 있습니다. | **(높음)** OOM 발생 시 이 값을 늘려 `batch_size`를 보완할 수 있습니다. |
| `lora_rank` (`r`) | LoRA 행렬의 랭크(차원)입니다. 값이 클수록 더 많은 파라미터를 학습하여 성능이 향상될 수 있지만, 메모리 사용량도 증가합니다. | **(중간)** OOM 발생 시 이 값을 줄여(예: 16 -> 8) 메모리를 확보할 수 있습니다. |
| `max_prompt_length` | 모델에 입력되는 프롬프트의 최대 길이입니다. | **(낮음)** 값이 길어지면 메모리 사용량이 증가합니다. |
| `max_completion_length` | 모델이 생성하는 답변의 최대 길이입니다. | **(낮음)** 생성 길이가 길어질수록 더 많은 VRAM이 필요합니다. |
| `bf16=True` | `bfloat16` 혼합 정밀도(mixed-precision) 훈련을 활성화합니다. `fp32`에 비해 메모리 사용량을 절반으로 줄이고 연산 속도를 높여줍니다. | **(중간)** OOM 방지를 위해 필수적으로 사용합니다. |
| `optim="adamw_bnb_8bit"` | 8-bit 양자화된 옵티마이저를 사용하여 옵티마이저 상태(optimizer states)가 차지하는 메모리를 줄입니다. | **(중간)** 메모리 효율적인 훈련을 위한 핵심 설정입니다. |

---

## 🆘 OOM (Out-of-Memory) 에러 해결 가이드

GPU 메모리 부족(OOM) 에러가 발생하면 다음 순서대로 파라미터를 조정해 보세요.

1.  **`per_device_train_batch_size` 줄이기** (예: 4 → 2 → 1)
    -   가장 효과적이고 즉각적인 해결책입니다.
2.  **`gradient_accumulation_steps` 늘리기** (예: 1 → 2 → 4)
    -   줄어든 배치 사이즈의 효과를 보완하기 위해 사용합니다. (실질적 배치 사이즈 = `batch_size` * `accumulation_steps`)
3.  **`lora_rank` 줄이기** (예: 16 → 8)
    -   학습 파라미터 수를 줄여 메모리 사용량을 감소시킵니다. (성능 저하 가능성 있음)
4.  **`max_prompt_length` 및 `max_completion_length` 줄이기** (예: 256 → 128)
    -   처리하는 시퀀스 길이를 줄여 메모리 부담을 완화합니다.

> **참고**: 이 코드는 이미 **4-bit 양자화**와 **8-bit 옵티마이저**를 사용하여 메모리를 크게 절약하고 있으므로, 위의 파라미터 튜닝으로 대부분의 OOM 문제를 해결할 수 있습니다.

---

## 🔬 접근 방식 1: 단순 강화학습 (Baseline)

### 개요
-   **목표**: 정답 일치, 형식 준수 등 명확한 규칙에 기반한 보상 시스템을 구축하여 기본적인 강화학습 모델의 성능을 측정합니다.
-   **보상 함수**:
    -   `correctness_reward_func`: 정답 숫자 일치 여부.
    -   `strict_format_reward_func`: 엄격한 XML 형식 준수 여부.
    -   `soft_format_reward_func`: 유연한 XML 형식 준수 여부.
    -   `int_reward_func`: 답변에 숫자 포함 여부.
    -   `xmlcount_reward_func`: XML 태그 존재 여부.

-   **한계**: 규칙으로 정의하기 어려운 풀이 과정의 논리적 오류나 창의적인 접근법을 평가하지 못합니다.

`2025년 KNS 춘계 학술발표회에 발표한 연구는 이 방식에 해당합니다. 논문과 발표 자료를 참고하시려면 아래 링크를 클릭하세요.`

[2025년 KNS 춘계 학술발표회 논문 발표 자료](./paper_and_poster/KNS_발표자료(포스터).pptx)

[2025년 KNS 춘계 학술발표회 제출 논문](./paper_and_poster/KNS_제출논문.pdf)

---

---

## 🔬 접근 방식 2: 외부 LLM을 활용한 보상 재설계 (Reward Shaping)

이 접근 방식은 단순한 규칙 기반 보상의 한계를 넘어서기 위해 **보상 재설계(Reward Shaping)**라는 정교한 기법을 도입합니다.

### 보상 재설계(Reward Shaping)란?

보상 재설계는 최종 결과(성공/실패)에 대해서만 보상을 주는 대신, 목표에 도달하는 **과정 중에 추가적인 보상 신호**를 주어 에이전트의 학습을 더 효율적으로 안내하는 기법입니다. 

여기서 정답 일치 여부과 같은 최종 결과에 대해서 보상을 주는 것 뿐만 아니라 풀이과정에 대해서 추가적인 보상을 주는 것을 `Reward Shaping`이라고 표현했다고 보시면 됩니다.

*   **희소한 보상(Sparse Reward) 문제**: 수학 문제 풀이에서 최종 답이 맞았을 때만 `+1`점, 틀렸을 때 `0`점을 준다고 가정해 봅시다. 모델은 수많은 단어를 생성하고 복잡한 논리 전개를 펼친 끝에 단 하나의 보상 신호를 받게 됩니다. 이는 학습이 비효율적이고, 모델이 "무엇을 잘했고 무엇을 못했는지" 알기 어려운 **'희소한 보상'** 문제입니다.
    
*   **조밀한 보상(Dense Reward) 제공**: 보상 재설계는 이 문제를 해결하기 위해 **"풀이 과정은 논리적이었는가?", "접근 방식이 효율적이었는가?"** 와 같은 중간 과정의 품질을 평가하여 더 **'조밀한 보상'**을 제공합니다. 이는 모델이 올바른 방향으로 나아가도록 세밀하게 안내하는 역할을 합니다.

### 워크플로우: Gemini를 지능적 평가자(Judge)로 활용하기

이 프로젝트에서는 강력한 외부 LLM인 `Gemini`를 지능적인 평가자로 사용하여 풀이 과정의 품질을 평가하고, 이를 보상 신호로 변환합니다. 전체적인 흐름은 다음과 같습니다.

1.  🤖 **풀이 생성 (Action)**
    
    *   훈련 대상인 **`AtomicGPT`(Student)**가 주어진 수학 문제에 대해 `reasoning(추론과정)과 answer(정답)`를 포함한 답변을 생성합니다.

 2.  🧐 **풀이 과정 추출**
     
     *   `reasoning_quality_reward_func` 함수는 생성된 답변에서 `<reasoning>` 태그 안의 텍스트, 즉 모델의 문제 해결 논리를 추출합니다.
         
 3.  📝 **평가 프롬프트 구성**
     
     *   추출된 풀이 과정, 원본 문제, 그리고 실제 정답을 조합하여 **`Gemini`(Judge)**에게 보낼 상세한 평가 프롬프트를 동적으로 생성합니다. 이 프롬프트는 Gemini에게 '수학 교육 전문가'의 역할을 부여하고, 명확한 채점 기준(논리성, 효율성 등)에 따라 0~5점 척도로 점수를 매기도록 지시합니다.
         
 4.  ⚖️ **외부 평가 수행**
     
     *   구성된 프롬프트를 `Gemini` API로 전송하여 평가를 요청합니다. `Gemini`는 주어진 기준에 따라 학생의 풀이 과정을 분석하고, 평가 결과를 JSON 형식으로 반환합니다.
         
 5.  📊 **보상 신호 변환**
     
     *   API 응답(JSON)에서 `reasoning_score`를 파싱합니다. 이 점수는 이제 "풀이 과정의 품질"을 나타내는 정량적 지표가 됩니다.
         
     *   이 점수를 0~1 사이로 정규화하고, `correctness_reward_func` (정답 여부) 및 `format_reward_func` (형식 준수) 등 다른 보상들과 가중합(`combined_reward_func`)하여 최종 "재설계된 보상(Shaped Reward)"을 계산합니다.
         
 6.  💡 **정책 업데이트 (Learning)**
     
     *   최종적으로 계산된 이 **재설계된 보상**을 사용하여 GRPO 알고리즘을 통해 `AtomicGPT`의 정책(모델의 LoRA 가중치)을 업데이트합니다.
 
 ### 장점과 한계
 
 | 장점 👍 | 한계 👎 |
 | :--- | :--- |
 | **고품질 피드백**: "논리성"이나 "창의성"처럼 규칙으로 정의하기 어려운 추상적인 품질을 평가할 수 있습니다. | **API 비용**: 외부 API를 호출할 때마다 비용이 발생하여 전체 훈련 비용이 증가합니다. |
 | **성능 향상 잠재력**: 인간 전문가의 평가와 유사한 수준의 피드백은 모델 성능을 획기적으로 향상시킬 수 있습니다. | **훈련 속도 저하**: API 호출에 수반되는 네트워크 지연(latency)으로 인해 훈련 파이프라인의 속도가 느려집니다. |
 | **학습 효율성**: 조밀한 보상 신호는 모델이 더 빠르고 안정적으로 올바른 방향을 학습하도록 돕습니다. | **외부 서비스 의존성**: Gemini 서비스의 상태나 정책 변경에 따라 훈련이 영향을 받을 수 있습니다. |
 
 이 접근 방식은 최고의 성능을 목표로 할 때 꽤나 합리적인 방법론이지만, 비용과 속도라는 현실적인 트레이드오프를 고려해야 합니다.
 
 ---


## 🔬 접근 방식 3: On-Premise Judge 모델 (Self-Contained RL)

### 개요
-   **목표**: 외부 API 의존성을 완전히 제거하고, 내부 서버(On-Premise)에서 실행되는 자체 Judge 모델을 활용하여 **데이터 보안, 비용 효율성, 완전한 통제권**을 확보합니다.
-   **핵심 아키텍처**:
    -   **Student 모델**: 훈련 대상 모델 (`KAERI-MLP/gemma2-Korean-AtomicGPT-9B`). 4-bit 양자화와 LoRA를 적용하여 효율적으로 학습합니다.
    -   **Judge 모델**: 평가자 모델 (`meta-llama/Llama-3.1-8B-Instruct`). Student 모델과 동일한 GPU 환경에서 로드되어, 생성된 답변을 실시간으로 평가하고 보상을 생성합니다. Judge 모델은 다른 모델을 활용해도 괜찮습니다. `Qwen/Qwen3-8B` 모델도 추천드립니다.

### 워크플로우

1.  🤖 **응답 생성**: **Student 모델**이 수학 문제에 대한 풀이 과정을 생성합니다.
2.  📝 **평가 프롬프트 구성**: `agent_reward_func` 함수가 Student의 답변, 원본 문제, 정답을 조합하여 **내부 Judge 모델**을 위한 평가 프롬프트를 생성합니다.
3.  ⚖️ **내부 평가 수행**: 구성된 프롬프트를 **Judge 모델**에 입력하여 추론을 실행합니다. Judge는 별도의 API 호출 없이 동일한 `cuda:0` 장치에서 직접 답변을 평가하고 점수를 생성합니다.
4.  📊 **보상 변환**: Judge가 반환한 점수를 0~1 사이의 값으로 정규화하여 최종 보상 신호로 사용합니다.
5.  🏆 **정책 업데이트 및 모델 저장**:
    -   정규화된 보상을 `GRPOTrainer`에 전달하여 **Student 모델**의 LoRA 가중치를 업데이트합니다.
    -   `BestModelSaver` 콜백이 매 스텝의 평균 보상을 추적하여, 역대 최고 평균 누적 보상을 기록할 때마다 현재 모델의 LoRA 어댑터를 `./best_adapter` 디렉토리에 자동으로 저장합니다.

### 코드 핵심 분석

-   **`agent_reward_func(...)`**: On-Premise 평가의 핵심 로직입니다. 외부 API 대신 내부 `judge_model`을 호출하여 보상을 계산합니다.
-   **`BestModelSaver` Callback**: 훈련 과정에서 최상의 성능을 보인 모델을 자동으로 저장하여, 최적의 체크포인트를 놓치지 않도록 돕는 매우 유용한 유틸리티입니다.
-   **로깅**: 모든 평가 과정(Judge에게 전달된 프롬프트, Judge의 원시 출력)은 `judge_evaluations_metric.jsonl` 파일에 기록되어, 평가 품질을 디버깅하고 분석하는 데 사용됩니다.

### 도전 과제 및 고려사항

-   **VRAM 관리**: **두 개의 대규모 언어 모델(Student, Judge)을 단일 GPU에 동시에 로드**하는 것은 상당한 VRAM을 요구합니다. Student 모델은 4-bit 양자화를 적용했지만, Judge 모델의 메모리 사용량도 고려해야 합니다. OOM 발생 시 Judge 모델에도 양자화를 적용하거나 더 작은 Judge 모델을 사용하는 것을 고려해야 합니다.
-   **훈련 속도**: 내부 Judge 모델의 추론 속도가 전체 훈련 파이프라인의 병목 현상(bottleneck)이 될 수 있습니다. 이는 외부의 고도로 최적화된 API보다 느릴 수 있습니다.
-   **Judge 모델의 품질**: 전체 시스템의 성능이 이제 내부 Judge 모델의 성능에 크게 의존합니다. Judge 모델 자체가 특정 태스크에 대해 편향되거나 성능이 낮을 경우, Student 모델의 학습이 잘못된 방향으로 유도될 수 있습니다.

---

## 📊 접근 방식 비교 요약

| 특징 | 접근 방식 1 (단순 RL) | 접근 방식 2 (외부 API) | 접근 방식 3 (On-Premise) |
| :--- | :--- | :--- | :--- |
| **보상 소스** | 규칙 기반 (정답, 형식) | 외부 LLM (Gemini) | 내부 LLM (Llama 3.1) |
| **평가 품질** | 낮음 | 높음 | 중간(Judge 성능에 의존) |
| **비용** | 없음 | 높음 (API 호출 비용) | 낮음 (초기 인프라 구축 비용) |
| **속도** | 빠름 | 느림 (네트워크 지연) | 중간 (내부 추론 속도에 의존) |
| **데이터 보안** | 높음 | 낮음 (데이터 외부 전송) | 높음 |
| **통제권** | 완전 통제 | 제한적 (API에 의존) | 완전 통제 |

